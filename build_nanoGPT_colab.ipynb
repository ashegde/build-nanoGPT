{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8OXvJ29i1TG8bGUX2uMF8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AVAuvocShbJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74386f8a-4d2b-4d56-918a-383ad5ae2ab3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-20 21:17:16--  https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5182 (5.1K) [text/plain]\n",
            "Saving to: ‘model.py’\n",
            "\n",
            "model.py            100%[===================>]   5.06K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-20 21:17:17 (50.1 MB/s) - ‘model.py’ saved [5182/5182]\n",
            "\n",
            "--2024-06-20 21:17:17--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-06-20 21:17:17 (18.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#!git clone https://github.com/ashegde/build-nanoGPT\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/model.py\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tiktoken\n",
        "\n",
        "from model import GPT, GPTConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a peak at the dataset\n",
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "data = text[:1000]\n",
        "print(data[:100])"
      ],
      "metadata": {
        "id": "p0pgIrhKjHyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a5e515-7e7f-4ba1-e775-6397517ce0eb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(data)\n",
        "print(tokens[:25])"
      ],
      "metadata": {
        "id": "baFlhc2FNg0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f277eccc-98a6-4fb5-abd3-8997755b66b1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT(GPTConfig())"
      ],
      "metadata": {
        "id": "avDIwrWcgT75"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation code block for the randomly initialized GPT2 model\n",
        "x = # (B,)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "while x.size(1) < max_length:\n",
        "  with torch.no_grad():\n",
        "    logits = model(x) # (B,T,vocab_size)\n",
        "    logits = logits[:, -1, :] #predictive distribution for the final token\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "    ix = torch.multinomial(topk_probs,1) # (B,1)\n",
        "    xcol = torch.gather(topk_indices, -1, ix) # (B,1)\n",
        "    x = torch.cat((x,xcol), dim=1)"
      ],
      "metadata": {
        "id": "12o1kA43ijLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(num_return_sequences):\n",
        "  tokens = x[i, :max_length].tolist()\n",
        "  decoded = enc.decode(tokens)\n",
        "  print(\">\", decoded)"
      ],
      "metadata": {
        "id": "n2CqrfPd1YR8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}