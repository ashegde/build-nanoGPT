{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGK48VvfWKKy4+MhiYU2cN"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AVAuvocShbJE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8b72fd1-933a-4f58-cc7e-b4ad40efaf41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-22 01:55:30--  https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/model.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5548 (5.4K) [text/plain]\n",
            "Saving to: ‘model.py’\n",
            "\n",
            "model.py            100%[===================>]   5.42K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-06-22 01:55:30 (54.9 MB/s) - ‘model.py’ saved [5548/5548]\n",
            "\n",
            "--2024-06-22 01:55:30--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-06-22 01:55:31 (26.6 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.6.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#!git clone https://github.com/ashegde/build-nanoGPT\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/model.py\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tiktoken\n",
        "\n",
        "from model import GPT, GPTConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "l4QMjGC1dhPN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a peak at the dataset\n",
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "data = text[:1000]\n",
        "print(data[:100])"
      ],
      "metadata": {
        "id": "p0pgIrhKjHyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950c709e-257d-4d5b-e583-0279d0464e40"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(data)\n",
        "print(tokens[:25])"
      ],
      "metadata": {
        "id": "baFlhc2FNg0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "233a67b9-9054-446c-de38-8611ce6eefb0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract a batch of tokens\n",
        "B, T = 4, 32\n",
        "tokens = enc.encode(text[:1000])\n",
        "buff = torch.tensor(tokens[:B*T+1])\n",
        "x = buff[:-1].view(B,T)\n",
        "y = buff[1:].view(B,T)"
      ],
      "metadata": {
        "id": "2FyBjfNrZl3-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a randomly initialized GPT model.\n",
        "model = GPT(GPTConfig())\n",
        "model.eval()\n",
        "device = 'cpu' #'cuda' if torch.cuda.is_available else 'cpu'\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "avDIwrWcgT75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a78c3bf2-2266-4243-a59f-0fe084313702"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='tanh')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter count\n",
        "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has a total of {num_parameters} parameters.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O02m4-nnrStI",
        "outputId": "28ea3140-746a-4103-8e05-bc456b36bd7e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has a total of 163037184 parameters.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generation code block for the randomly initialized GPT2 model\n",
        "num_return_seqs = 5\n",
        "max_length = 30\n",
        "\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\") # (B,)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) # (B,)\n",
        "tokens = tokens[None,:].repeat(num_return_seqs, 1) # (5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "  with torch.no_grad():\n",
        "    logits = model(x) # (B,T,vocab_size)\n",
        "    logits = logits[:, -1, :] #predictive distribution for the final token\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "    ix = torch.multinomial(topk_probs,1) # (B,1)\n",
        "    xcol = torch.gather(topk_indices, -1, ix) # (B,1)\n",
        "    x = torch.cat((x,xcol), dim=1)"
      ],
      "metadata": {
        "id": "12o1kA43ijLO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoding the generated text\n",
        "for i in range(num_return_seqs):\n",
        "  tokens = x[i, :max_length].tolist()\n",
        "  decoded = enc.decode(tokens)\n",
        "  print(\">\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJcSH15pcy0u",
        "outputId": "91d7e1ec-84d5-4ac9-dcb8-b81e90f8fd89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, welding outfit trophies setupsypes cruiser complexities��rienTedل intrigued preced CommunismakedownTOR number advertisements unfitMcFinish Fund\n",
            "> Hello, I'm a language model,32 Dum opticterrorism Venezuel Action apartimilaryersurgy09novtale Profiturgical fractureddemaut reaction turn section Archive\n",
            "> Hello, I'm a language model, republican satire Chrom Cant 136 visualization Investments Goddardushed STATES underneath Wiley Transactionsetch Renewfoot GreenwaldMex skimAmbinspiredenvironment\n",
            "> Hello, I'm a language model, Feng EP underwater Cant hipthirds incomingTruth Unicorn ). retention orgasm pursued Ballistic Desc easiestarsh Bradford Blackburn spouses scourgeresses\n",
            "> Hello, I'm a language model,Continueinates Administratorisitionsairs operationCruz meg thinner Hamilton trucks delayedineseistedffic clubproxy variation regulators GasadeonPK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Single forward pass through the model, from data to loss\n",
        "\n",
        "cfg = GPTConfig()\n",
        "model = GPT(cfg)\n",
        "model.eval()\n",
        "device = \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "B, T = 4, 32\n",
        "tokens = enc.encode(text[:1000])\n",
        "buff = torch.tensor(tokens[:B*T+1])\n",
        "x = buff[:-1].view(B,T)\n",
        "y = buff[1:].view(B,T)\n",
        "\n",
        "#with torch.no_grad():\n",
        "logits, loss = model(x, y)\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rwfnU6Pbcue",
        "outputId": "922badc8-b231-4ab9-dd94-30faecba4eab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11.0165, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# nats needed to describe the vocab_size\n",
        "# this is roughly on par with the untrained loss\n",
        "np.log(cfg.vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyAVP3qbq-kT",
        "outputId": "c8ebb47d-27b6-4600-960a-17c5c02f4c38"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.82490511970208"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}