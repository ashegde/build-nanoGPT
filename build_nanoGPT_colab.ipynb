{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiMeK6b+5t3KUNbKuXhS25"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVAuvocShbJE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#!git clone https://github.com/ashegde/build-nanoGPT\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/model.py\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/loader.py\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/utils.py\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tiktoken\n",
        "\n",
        "from model import GPT, GPTConfig\n",
        "from loader import DataLoaderLite\n",
        "from utils import LinearWarmupCosineAnnealingScheduler, say_hello\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "l4QMjGC1dhPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trial optimization loop -- overfitting on a single batch (same batch as above)\n",
        "cfg = GPTConfig(vocab_size=50304) #overriding vocab_size with a power of 2\n",
        "model = GPT(cfg)\n",
        "model.train()\n",
        "device = \"cpu\"\n",
        "model.to(device)\n",
        "# parameter count\n",
        "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has a total of {num_parameters} parameters.')\n",
        "\n",
        "model = torch.compile(model) # most of the benefit here may be for GPUs"
      ],
      "metadata": {
        "id": "4RoO0hjDQWKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nats needed to describe the vocab_size\n",
        "# this is roughly on par with the untrained loss\n",
        "np.log(cfg.vocab_size)"
      ],
      "metadata": {
        "id": "DnwEZOOAVh3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size and gradient accumulator setup\n",
        "\n",
        "total_batch_size = 524288 #2**19 = roughly the 0.5M token batch size listed in the GPT3 paper regarding the size of the 125M parameter GPT2\n",
        "B = 16 # \"micro\"-batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T) == 0, \"total_batch_size should be divisble by B * T\"\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "train_loader = DataLoaderLite(B,T)\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\" required gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_load = DataLoaderLite(B=B, T=T)\n"
      ],
      "metadata": {
        "id": "2DtRr52vWqoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# revised training loop\n",
        "\n",
        "\n",
        "\n",
        "# torch.set_float32_matmul_precision('high') # need cuda\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9,0.95), eps=1e-8)\n",
        "optimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate=6e-4, device=device)\n",
        "\n",
        "max_lr = 3e-4\n",
        "min_lr = 0.1 * max_lr\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "scheduler = LinearWarmupCosineAnnealingScheduler(max_lr=max_lr, min_lr=min_lr, warmup_steps=warmup_steps, max_steps=max_steps)\n",
        "\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss_accum = 0\n",
        "  for micro_steps in range(grad_accum_steps):\n",
        "    x, y = train_loader.next_batch()\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "      logits, loss = model(x,y)\n",
        "    loss = loss / grad_accum_steps # compensate for accumulation, the loss defaults to a mean reduction\n",
        "    loss_accum += loss.detach()\n",
        "    loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  # lr\n",
        "  lr = scheduler.get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  #torch.cuda.synchronizer() # need cuda\n",
        "  t1 = time.time()\n",
        "  elapsed = t1-t0\n",
        "\n",
        "  print(f'step {step} || loss: {loss_accum.item():} || norm: {norm:0.4f} || time elapsed: dt={elapsed*1000:0.4f}ms')\n",
        "\n"
      ],
      "metadata": {
        "id": "zql052d80ZiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "say_hello(model)"
      ],
      "metadata": {
        "id": "LXtPLPpw3_Wm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}