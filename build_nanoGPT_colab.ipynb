{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrwC3R5e3X27fN84d1EDFs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVAuvocShbJE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#!git clone https://github.com/ashegde/build-nanoGPT\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/model.py\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/loader.py\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/utils.py\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tiktoken\n",
        "\n",
        "from model import GPT, GPTConfig\n",
        "from loader import DataLoaderLite\n",
        "from utils import LinearWarmupCosineAnnealingScheduler, say_hello\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "l4QMjGC1dhPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trial optimization loop -- overfitting on a single batch (same batch as above)\n",
        "cfg = GPTConfig(vocab_size=50304) #overriding vocab_size with a power of 2\n",
        "model = GPT(cfg)\n",
        "model.train()\n",
        "device = \"cpu\"\n",
        "model.to(device)\n",
        "# parameter count\n",
        "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has a total of {num_parameters} parameters.')\n",
        "\n",
        "model = torch.compile(model) # most of the benefit here may be for GPUs"
      ],
      "metadata": {
        "id": "4RoO0hjDQWKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nats needed to describe the vocab_size\n",
        "# this is roughly on par with the untrained loss\n",
        "np.log(cfg.vocab_size)"
      ],
      "metadata": {
        "id": "DnwEZOOAVh3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch size and gradient accumulator setup\n",
        "\n",
        "total_batch_size = 524288 #2**19 = roughly the 0.5M token batch size listed in the GPT3 paper regarding the size of the 125M parameter GPT2\n",
        "B = 16 # \"micro\"-batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T) == 0, \"total_batch_size should be divisble by B * T\"\n",
        "grad_accum_steps = total_batch_size // (B * T)\n",
        "train_loader = DataLoaderLite(B,T)\n",
        "print(f\"total desired batch size: {total_batch_size}\")\n",
        "print(f\" required gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_load = DataLoaderLite(B=B, T=T)\n"
      ],
      "metadata": {
        "id": "2DtRr52vWqoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# revised training loop\n",
        "\n",
        "\n",
        "\n",
        "# torch.set_float32_matmul_precision('high') # need cuda\n",
        "#optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9,0.95), eps=1e-8)\n",
        "optimizer = model.configure_optimizers(weight_decay = 0.1, learning_rate=6e-4, device=device)\n",
        "\n",
        "max_lr = 3e-4\n",
        "min_lr = 0.1 * max_lr\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "scheduler = LinearWarmupCosineAnnealingScheduler(max_lr=max_lr, min_lr=min_lr, warmup_steps=warmup_steps, max_steps=max_steps)\n",
        "\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss_accum = 0\n",
        "  for micro_steps in range(grad_accum_steps):\n",
        "    x, y = train_loader.next_batch()\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "      logits, loss = model(x,y)\n",
        "    loss = loss / grad_accum_steps # compensate for accumulation, the loss defaults to a mean reduction\n",
        "    loss_accum += loss.detach()\n",
        "    loss.backward()\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  # lr\n",
        "  lr = scheduler.get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  #torch.cuda.synchronizer() # need cuda\n",
        "  t1 = time.time()\n",
        "  elapsed = t1-t0\n",
        "\n",
        "  print(f'step {step} || loss: {loss_accum.item():} || norm: {norm:0.4f} || time elapsed: dt={elapsed*1000:0.4f}ms')\n",
        "\n"
      ],
      "metadata": {
        "id": "zql052d80ZiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "say_hello(model)"
      ],
      "metadata": {
        "id": "LXtPLPpw3_Wm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distributed-Data Parallel\n",
        "\n",
        "Suppose we place the training routine in a script called 'train_script.py'.\n",
        "\n",
        "Then, running `python train_script.py` via command line initiates training in the standard fashion (on a single GPU, should one be available).\n",
        "\n",
        "Instead, we can launch the script with DDP across -- e.g., 8 GPUs -- via the command `torchrun --standalone --nproc_per_node=8 train_script.py`"
      ],
      "metadata": {
        "id": "y3VgJwsWSbkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DDP-based training loop\n",
        "from torch.distributed import init_process_group, destroy_process_group\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import torch.distributed as dist\n",
        "# setup the DDP environment\n",
        "# `torchrun` command sets the env variables RANK, LOCAL_RANK, and WORLD_SIZE\n",
        "\n",
        "ddp = int(os.environ.get('RANK', -1)) != -1 # is DDP run?\n",
        "if ddp:\n",
        "  assert torch.cuda.is_available(), \"DDP requires CUDA\"\n",
        "  init_process_group(backend='nccl')\n",
        "  ddp_rank = int(os.environ['RANK'])\n",
        "  ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
        "  ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
        "  device = f'cuda:{ddp_local_rank}' # cuda device\n",
        "  torch.cuda.set_device(device)\n",
        "  master_process = ddp_rank == 0 # this \"master\" process will do checkpointing, logging, etc.\n",
        "else:\n",
        "  # non-DDP run\n",
        "  ddp_rank = 0\n",
        "  ddp_local_rank = 0\n",
        "  ddp_world_size = 1\n",
        "  master_process =True\n",
        "  # auto-detect device\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  print(f\"using device: {device}\")\n",
        "\n",
        "seedval = 1337\n",
        "torch.manual_seed(seedval)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed(seedval)\n",
        "\n",
        "# batch size and gradient accumulator setup with DDP\n",
        "total_batch_size = 524288 #2**19 = roughly the 0.5M token batch size listed in the GPT3 paper regarding the size of the 125M parameter GPT2\n",
        "# each of the `WORLD_SIZE` processes will use the following B and T.\n",
        "# thus, each forward pass through the model (during training) will process B * T * ddp_world_size tokens\n",
        "B = 16 # \"micro\"-batch size\n",
        "T = 1024 # sequence length\n",
        "assert total_batch_size % (B * T * ddp_world_size) == 0, \"total_batch_size should be divisble by B * T * ddp_world_size\"\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "if master_process:\n",
        "  print(f\"total desired batch size: {total_batch_size}\")\n",
        "  print(f\" required gradient accumulation steps: {grad_accum_steps}\")\n",
        "\n",
        "train_loader = DataLoaderLite(B=B, T=T, process_rank=ddp_rank, num_processes=ddp_world_size)\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "\n",
        "# create model\n",
        "cfg = GPTConfig(vocab_size=50304) #overriding vocab_size with a power of 2\n",
        "model = GPT(cfg)\n",
        "model.to(device)\n",
        "model.train()\n",
        "model = torch.compile(model) # most of the benefit here may be for GPUs\n",
        "if ddp:\n",
        "  model = DDP(model, device_ids=[ddp_local_rank])\n",
        "\n",
        "raw_model = model.module if ddp else model\n",
        "\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = 0.1 * max_lr\n",
        "warmup_steps = 10\n",
        "max_steps = 50\n",
        "\n",
        "optimizer = raw_model.configure_optimizers(weight_decay = 0.1, learning_rate=max_lr, device=device)\n",
        "scheduler = LinearWarmupCosineAnnealingScheduler(max_lr=max_lr, min_lr=min_lr, warmup_steps=warmup_steps, max_steps=max_steps)\n",
        "\n",
        "for step in range(max_steps):\n",
        "  t0 = time.time()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  loss_accum = 0\n",
        "  for micro_step in range(grad_accum_steps):\n",
        "    x, y = train_loader.next_batch()\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "      logits, loss = model(x,y)\n",
        "    loss = loss / grad_accum_steps # compensate for accumulation, the loss defaults to a mean reduction\n",
        "    loss_accum += loss.detach()\n",
        "    if ddp:\n",
        "      model.require_backward_grad_sync = (micro_step == grad_accum_steps - 1)\n",
        "    loss.backward()\n",
        "  if ddp:\n",
        "    dist.all_reduce(loss_accum, op=dist.ReduceOp.AVG)\n",
        "  norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "  # lr\n",
        "  lr = scheduler.get_lr(step)\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr\n",
        "  optimizer.step()\n",
        "  #torch.cuda.synchronizer() # need cuda\n",
        "  t1 = time.time()\n",
        "  dt = t1-t0\n",
        "  if master_process:\n",
        "    print(f'step {step} || loss: {loss_accum.item():} || norm: {norm:0.4f} || time elapsed: dt={dt*1000:0.4f}ms')\n",
        "\n",
        "if ddp:\n",
        "  destroy_process_group()\n"
      ],
      "metadata": {
        "id": "EIwsyhABvOi6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}