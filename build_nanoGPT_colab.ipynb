{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqac4mLzVBE2t/Jf1L1Omq"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVAuvocShbJE"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "#!git clone https://github.com/ashegde/build-nanoGPT\n",
        "!wget https://raw.githubusercontent.com/ashegde/build-nanoGPT/main/model.py\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "!pip install tiktoken\n",
        "\n",
        "from model import GPT, GPTConfig\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ],
      "metadata": {
        "id": "l4QMjGC1dhPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# taking a peak at the dataset\n",
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "data = text[:1000]\n",
        "print(data[:100])"
      ],
      "metadata": {
        "id": "p0pgIrhKjHyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = tiktoken.get_encoding('gpt2')\n",
        "tokens = enc.encode(data)\n",
        "print(tokens[:25])"
      ],
      "metadata": {
        "id": "baFlhc2FNg0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract a batch of tokens\n",
        "B, T = 4, 32\n",
        "tokens = enc.encode(text[:1000])\n",
        "buff = torch.tensor(tokens[:B*T+1])\n",
        "x = buff[:-1].view(B,T)\n",
        "y = buff[1:].view(B,T)"
      ],
      "metadata": {
        "id": "2FyBjfNrZl3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a randomly initialized GPT model.\n",
        "model = GPT(GPTConfig())\n",
        "model.eval()\n",
        "device = 'cpu' #'cuda' if torch.cuda.is_available else 'cpu'\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "avDIwrWcgT75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# parameter count\n",
        "num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'The model has a total of {num_parameters} parameters.')"
      ],
      "metadata": {
        "id": "O02m4-nnrStI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generation code block for the randomly initialized GPT2 model\n",
        "num_return_seqs = 5\n",
        "max_length = 30\n",
        "\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\") # (B,)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long) # (B,)\n",
        "tokens = tokens[None,:].repeat(num_return_seqs, 1) # (5, 8)\n",
        "x = tokens.to(device)\n",
        "\n",
        "while x.size(1) < max_length:\n",
        "  with torch.no_grad():\n",
        "    logits = model(x) # (B,T,vocab_size)\n",
        "    logits = logits[:, -1, :] #predictive distribution for the final token\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "    ix = torch.multinomial(topk_probs,1) # (B,1)\n",
        "    xcol = torch.gather(topk_indices, -1, ix) # (B,1)\n",
        "    x = torch.cat((x,xcol), dim=1)"
      ],
      "metadata": {
        "id": "12o1kA43ijLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# decoding the generated text\n",
        "for i in range(num_return_seqs):\n",
        "  tokens = x[i, :max_length].tolist()\n",
        "  decoded = enc.decode(tokens)\n",
        "  print(\">\", decoded)"
      ],
      "metadata": {
        "id": "bJcSH15pcy0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Single forward pass through the model, from data to loss\n",
        "\n",
        "cfg = GPTConfig()\n",
        "model = GPT(cfg)\n",
        "model.eval()\n",
        "device = \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "with open('input.txt', 'r') as f:\n",
        "  text = f.read()\n",
        "\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding('gpt2')\n",
        "B, T = 4, 32\n",
        "tokens = enc.encode(text[:1000])\n",
        "buff = torch.tensor(tokens[:B*T+1])\n",
        "buff = buff.to(device)\n",
        "x = buff[:-1].view(B,T)\n",
        "y = buff[1:].view(B,T)\n",
        "\n",
        "#with torch.no_grad():\n",
        "logits, loss = model(x, y)\n",
        "\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "2rwfnU6Pbcue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nats needed to describe the vocab_size\n",
        "# this is roughly on par with the untrained loss\n",
        "np.log(cfg.vocab_size)"
      ],
      "metadata": {
        "id": "tyAVP3qbq-kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trial optimization loop -- overfitting on a single batch (same batch as above)\n",
        "cfg = GPTConfig()\n",
        "model = GPT(cfg)\n",
        "model.train()\n",
        "device = \"cpu\"\n",
        "model.to(device)\n",
        "\n",
        "max_iter = 50\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for ii in range(max_iter):\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f'step {ii} || loss: {loss.item():}')\n",
        "\n"
      ],
      "metadata": {
        "id": "Gn3k0jVLi08q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we have \"overfitted\" to a single batch. Let's see how the model generates text again.\n",
        "\n",
        "def say_hello(model):\n",
        "  model.eval()\n",
        "\n",
        "  num_return_seqs = 5\n",
        "  max_length = 30\n",
        "\n",
        "  enc = tiktoken.get_encoding('gpt2')\n",
        "  tokens = enc.encode(\"Hello, I'm a language model,\") # (B,)\n",
        "  tokens = torch.tensor(tokens, dtype=torch.long) # (B,)\n",
        "  tokens = tokens[None,:].repeat(num_return_seqs, 1) # (5, 8)\n",
        "  x = tokens.to(device)\n",
        "  while x.size(1) < max_length:\n",
        "    with torch.no_grad():\n",
        "      logits, _ = model(x) # (B,T,vocab_size)\n",
        "      logits = logits[:, -1, :] #predictive distribution for the final token\n",
        "      probs = F.softmax(logits, dim=-1)\n",
        "      topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "      ix = torch.multinomial(topk_probs,1) # (B,1)\n",
        "      xcol = torch.gather(topk_indices, -1, ix) # (B,1)\n",
        "      x = torch.cat((x,xcol), dim=1)\n",
        "  # decoding the generated text\n",
        "  for i in range(num_return_seqs):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)\n",
        "\n",
        "#say_hello(model)"
      ],
      "metadata": {
        "id": "iOUtpqirly7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's build a simple dataloader\n",
        "\n",
        "class DataLoaderLite:\n",
        "  def __init__(self, B, T):\n",
        "    self.B = B\n",
        "    self.T = T\n",
        "\n",
        "    with open('input.txt', 'r') as f:\n",
        "      text = f.read()\n",
        "    enc = tiktoken.get_encoding('gpt2')\n",
        "    tokens = enc.encode(text)\n",
        "    self.tokens = torch.tensor(tokens)\n",
        "    print(f'loaded {len(self.tokens)} tokens')\n",
        "    print(f'1 epoch = {len(self.tokens) // (B*T)} batches')\n",
        "\n",
        "    # state (for iterating)\n",
        "    self.current_position = 0\n",
        "\n",
        "  def next_batch(self):\n",
        "    B, T = self.B, self.T\n",
        "    buff = self.tokens[self.current_position : self.current_position+B*T+1]\n",
        "    x = buff[:-1].view(B, T) # inputs\n",
        "    y = buff[1:].view(B, T) # targets\n",
        "    self.current_position += B*T\n",
        "\n",
        "    # reset the position if we cannot construct the next batch of inputs and targets\n",
        "    if self.current_position + (B*T+1) > len(self.tokens):\n",
        "      self.current_position = 0\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "C0mQxAONoH56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trial optimization loop -- overfitting on a single batch (same batch as above)\n",
        "cfg = GPTConfig()\n",
        "model = GPT(cfg)\n",
        "model.train()\n",
        "device = \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "4RoO0hjDQWKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# revised training loop\n",
        "\n",
        "# dataloader\n",
        "B = 4\n",
        "T = 32\n",
        "train_loader = DataLoaderLite(B,T)\n",
        "\n",
        "max_iter = 50\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "for ii in range(max_iter):\n",
        "  x, y = train_loader.next_batch()\n",
        "  x = x.to(device)\n",
        "  y = y.to(device)\n",
        "  optimizer.zero_grad()\n",
        "  logits, loss = model(x,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f'step {ii} || loss: {loss.item():}')\n",
        "\n"
      ],
      "metadata": {
        "id": "zql052d80ZiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "say_hello(model)"
      ],
      "metadata": {
        "id": "LXtPLPpw3_Wm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}